{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CHDModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natekchua/TensorFlow-Modelling/blob/master/Part3/CHDModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR5ZvHvHjckJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3umf1xCKjlMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKoSbkA80-8r",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "5491abf6-9902-4e05-d67a-5f87cb6bae97"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bd7e268c-d1bf-4267-a5dc-7e01f72a7c3c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-bd7e268c-d1bf-4267-a5dc-7e01f72a7c3c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving heart_test.csv to heart_test.csv\n",
            "Saving heart_train.csv to heart_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CnJowckjos7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_path = \"heart_train.csv\"\n",
        "test_file_path = \"heart_test.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsvcsC4Ojxy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make numpy values easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOzWZIMYj9w2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "3f7ccc68-6813-4858-c831-fe58b95c1169"
      },
      "source": [
        "!head {train_file_path}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "row.names,sbp,tobacco,ldl,adiposity,famhist,typea,obesity,alcohol,age,chd\r\n",
            "1,160,12,5.73,23.11,Present,49,25.3,97.2,52,1\r\n",
            "2,144,0.01,4.41,28.61,Absent,55,28.87,2.06,63,1\r\n",
            "3,118,0.08,3.48,32.28,Present,52,29.14,3.81,46,0\r\n",
            "4,170,7.5,6.41,38.03,Present,51,31.99,24.26,58,1\r\n",
            "5,134,13.6,3.5,27.78,Present,60,25.99,57.34,49,1\r\n",
            "6,132,6.2,6.47,36.21,Present,62,30.77,14.14,45,0\r\n",
            "7,142,4.05,3.38,16.2,Absent,59,20.81,2.62,38,0\r\n",
            "8,114,4.08,4.59,14.6,Present,62,23.11,6.72,58,1\r\n",
            "9,114,0,3.83,19.4,Present,49,24.86,2.49,29,0\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB5lIfk_kAtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABEL_COLUMN = 'chd'\n",
        "LABELS = [0, 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i-952UukDQh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0e8fc160-13e4-4236-faab-10420473568c"
      },
      "source": [
        "\n",
        "def get_dataset(file, **kwargs):\n",
        "  dataset = tf.data.experimental.make_csv_dataset(\n",
        "      file,\n",
        "      batch_size = 50,\n",
        "      label_name=LABEL_COLUMN,\n",
        "      na_value =\"?\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True,\n",
        "      **kwargs)\n",
        "  return dataset\n",
        "\n",
        "SELECT_COLUMNS = ['sbp','tobacco','ldl','adiposity','famhist', 'typea','obesity','alcohol','age','chd']\n",
        "raw_train_data = get_dataset(train_file_path, select_columns=SELECT_COLUMNS)\n",
        "raw_test_data = get_dataset(test_file_path, select_columns=SELECT_COLUMNS)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/data/experimental/ops/readers.py:521: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3l_XTRMkE19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_batch(dataset):\n",
        "  for batch, label in dataset.take(1):\n",
        "    for key, value in batch.items():\n",
        "      print(\"{:20s}: {}\".format(key,value.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLwwe8TY1oHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "134eef65-855c-4d6a-e43e-ccd05ef2df36"
      },
      "source": [
        "show_batch(raw_train_data)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sbp                 : [120 122 130 140 142 174 146 154 132 124 128 190 128 154 136 116 122 134\n",
            " 118 112 152 118 168 122 160 136 136 208 142 127 142 134 126 114 152 130\n",
            " 142 120 146 124 130 144 146 148 130 130 134 144 138 144]\n",
            "tobacco             : [ 3.7   0.    1.75  0.    0.    0.    3.6   0.    4.05 14.    4.65  4.18\n",
            "  0.    1.8   1.7   1.03  4.4   1.5   1.25  9.65  0.    0.12 11.4   3.2\n",
            " 12.    3.46  0.   27.4   7.44  0.    0.    0.    0.    3.6  19.45  0.\n",
            "  0.28 10.5   4.36  0.    7.28  0.04  0.    5.5   0.56  0.    6.4   0.01\n",
            "  1.15  0.76]\n",
            "ldl                 : [ 4.02  5.75  5.46  2.4   4.19  8.46  3.51  4.81  5.15  6.23  3.31  5.05\n",
            "  2.51  7.13  3.53  2.83  3.18  3.73  4.69  2.29  6.06  1.96  5.08  3.59\n",
            "  5.73  6.38  4.12  3.12  5.52  2.81  3.72  5.63  5.98  4.16  4.22  2.82\n",
            "  1.8   2.7   4.31  2.28  3.56  3.38  6.62  7.1   3.3   3.92  8.49  4.41\n",
            "  5.09 10.53]\n",
            "adiposity           : [39.66 30.9  34.34 27.89 18.04 35.1  22.67 28.11 26.51 35.96 22.74 24.83\n",
            " 29.35 34.04 20.13 10.85 11.59 21.53 31.58 17.2  41.05 20.31 26.66 22.49\n",
            " 23.11 32.25 17.42 26.63 33.97 15.7  25.68 29.12 29.06 22.58 29.81 19.63\n",
            " 21.03 29.87 18.44 24.86 23.29 23.61 25.69 25.31 30.86 25.55 37.25 28.61\n",
            " 27.87 35.66]\n",
            "famhist             : [b'Absent' b'Present' b'Absent' b'Present' b'Absent' b'Present' b'Absent'\n",
            " b'Present' b'Present' b'Present' b'Absent' b'Absent' b'Present'\n",
            " b'Present' b'Absent' b'Absent' b'Present' b'Absent' b'Present' b'Present'\n",
            " b'Present' b'Absent' b'Present' b'Present' b'Present' b'Present'\n",
            " b'Absent' b'Absent' b'Absent' b'Absent' b'Absent' b'Absent' b'Present'\n",
            " b'Absent' b'Absent' b'Present' b'Absent' b'Present' b'Present' b'Present'\n",
            " b'Present' b'Absent' b'Absent' b'Absent' b'Absent' b'Absent' b'Present'\n",
            " b'Absent' b'Present' b'Absent']\n",
            "typea               : [61 46 53 70 56 35 51 56 31 45 62 45 53 52 56 45 59 41 52 54 51 37 56 45\n",
            " 49 43 52 66 47 42 48 68 56 60 28 70 57 54 47 50 20 30 60 56 49 68 56 55\n",
            " 61 63]\n",
            "obesity             : [30.57 29.01 29.42 30.74 23.65 25.27 22.29 25.67 26.67 30.09 22.95 26.09\n",
            " 22.05 35.51 19.44 21.59 21.94 24.7  27.16 23.53 40.34 20.01 27.04 24.96\n",
            " 25.3  28.73 21.66 27.45 29.29 22.03 24.37 32.33 25.39 24.49 23.95 24.86\n",
            " 23.65 24.5  24.72 22.24 26.8  23.75 28.07 29.84 27.52 28.02 28.94 28.87\n",
            " 25.65 34.35]\n",
            "alcohol             : [  0.     4.11   0.   144.    20.78   0.    43.71  75.77  16.3    0.\n",
            "   0.51  82.85   1.37  39.36  14.4    1.75   0.    11.11   4.11   0.68\n",
            "   0.     2.42   2.61  36.17  97.2    3.13  12.86  33.07  24.27   1.03\n",
            "   5.25   2.02  11.52  65.31   0.     0.     2.93  16.46  10.8    8.26\n",
            "  51.87   4.66   8.23   3.6   33.33   0.68  10.49   2.06   2.34   0.  ]\n",
            "age                 : [64 42 58 29 42 61 42 59 50 59 48 41 62 44 55 21 33 30 53 53 51 18 59 58\n",
            " 52 43 40 62 54 17 40 34 64 31 59 29 33 49 38 38 58 30 63 48 45 27 51 63\n",
            " 44 55]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2of3_YhmxN4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, labels_batch = next(iter(raw_train_data)) \n",
        "test_batch, labels_batch = next(iter(raw_test_data)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLm3gm_lmzne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pack(features, label):\n",
        "  return tf.stack(list(features.values()), axis=-1), label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SbNfhJbm5y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PackNumericFeatures(object):\n",
        "  def __init__(self, names):\n",
        "    self.names = names\n",
        "\n",
        "  def __call__(self, features, labels):\n",
        "    numeric_features = [features.pop(name) for name in self.names]\n",
        "    numeric_features = [tf.cast(feat, tf.float32) for feat in numeric_features]\n",
        "    numeric_features = tf.stack(numeric_features, axis=-1)\n",
        "    features['numeric'] = numeric_features\n",
        "\n",
        "    return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZopkcb9m8vY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUMERIC_FEATURES = ['sbp','tobacco','ldl','adiposity', 'typea','obesity','alcohol','age']\n",
        "FEATURES = 9\n",
        "\n",
        "packed_train_data = raw_train_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))\n",
        "\n",
        "packed_test_data = raw_test_data.map(\n",
        "    PackNumericFeatures(NUMERIC_FEATURES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bI7e2gLnFL9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22b8ca57-e990-4cca-b339-0bdc104eb32a"
      },
      "source": [
        "show_batch(packed_train_data)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "famhist             : [b'Absent' b'Present' b'Absent' b'Absent' b'Present' b'Absent' b'Absent'\n",
            " b'Present' b'Absent' b'Present' b'Absent' b'Absent' b'Absent' b'Present'\n",
            " b'Present' b'Present' b'Absent' b'Absent' b'Absent' b'Absent' b'Present'\n",
            " b'Absent' b'Absent' b'Absent' b'Absent' b'Present' b'Absent' b'Present'\n",
            " b'Absent' b'Absent' b'Present' b'Absent' b'Absent' b'Present' b'Absent'\n",
            " b'Absent' b'Absent' b'Present' b'Present' b'Absent' b'Absent' b'Absent'\n",
            " b'Present' b'Absent' b'Present' b'Absent' b'Absent' b'Absent' b'Present'\n",
            " b'Present']\n",
            "numeric             : [[138.     0.6    3.81  28.66  54.    28.7    1.46  58.  ]\n",
            " [152.     0.     6.06  41.05  51.    40.34   0.    51.  ]\n",
            " [180.     0.52   4.23  16.38  55.    22.56  14.77  45.  ]\n",
            " [124.     0.     3.23   9.64  59.    22.7    0.    16.  ]\n",
            " [136.     8.8    4.26  32.03  52.    31.44  34.35  60.  ]\n",
            " [128.     0.     2.98  12.59  65.    20.74   2.06  19.  ]\n",
            " [128.     4.65   3.31  22.74  62.    22.95   0.51  48.  ]\n",
            " [126.     0.09   5.03  13.27  50.    17.75   4.63  20.  ]\n",
            " [168.     4.5    6.68  28.47  43.    24.25  24.38  56.  ]\n",
            " [114.     0.1    3.95  15.89  57.    20.31  17.14  16.  ]\n",
            " [126.     5.5    3.78  34.15  55.    28.85   3.18  61.  ]\n",
            " [152.     3.     4.64  31.29  41.    29.34   4.53  40.  ]\n",
            " [166.     3.     3.82  26.75  45.    20.86   0.    63.  ]\n",
            " [132.     7.9    2.85  26.5   51.    26.16  25.71  44.  ]\n",
            " [170.     4.2    4.67  35.45  50.    27.14   7.92  60.  ]\n",
            " [134.    11.79   4.01  26.57  38.    21.79  38.88  61.  ]\n",
            " [118.     1.5    5.38  25.84  64.    28.63   3.89  29.  ]\n",
            " [132.     7.     3.2   23.26  77.    23.64  23.14  49.  ]\n",
            " [136.     1.5    6.06  26.54  54.    29.38  14.5   33.  ]\n",
            " [118.     1.     5.76  22.1   62.    23.48   7.71  42.  ]\n",
            " [136.     7.5    7.39  28.04  50.    25.01   0.    45.  ]\n",
            " [122.     0.     3.76  24.59  56.    24.36   0.    30.  ]\n",
            " [144.     0.01   4.41  28.61  55.    28.87   2.06  63.  ]\n",
            " [101.     0.48   7.26  13.    50.    19.82   5.19  16.  ]\n",
            " [143.     5.04   4.86  23.59  58.    24.69  18.72  42.  ]\n",
            " [152.    10.1    4.71  24.65  65.    26.21  24.53  57.  ]\n",
            " [156.     3.     1.82  27.55  60.    23.91  54.    53.  ]\n",
            " [164.    13.02   6.26  29.38  47.    22.75  37.03  54.  ]\n",
            " [120.     7.5   15.33  22.    60.    25.31  34.49  49.  ]\n",
            " [126.    19.6    6.03  34.99  49.    26.99  55.89  44.  ]\n",
            " [146.     5.08   7.03  27.41  63.    36.46  24.48  37.  ]\n",
            " [158.     6.17   8.12  30.75  46.    27.84  92.62  48.  ]\n",
            " [158.    13.5    5.04  30.79  54.    24.79  21.5   62.  ]\n",
            " [148.     0.     5.32  26.71  52.    32.21  32.78  27.  ]\n",
            " [118.     0.12   1.96  20.31  37.    20.01   2.42  18.  ]\n",
            " [116.     3.     3.05  30.31  41.    23.63   0.86  44.  ]\n",
            " [130.     0.56   3.3   30.86  49.    27.52  33.33  45.  ]\n",
            " [138.     2.16   4.9   24.83  39.    26.06  28.29  29.  ]\n",
            " [136.     0.     5.    27.58  49.    27.59   1.47  39.  ]\n",
            " [138.     6.     7.24  37.05  38.    28.69   0.    59.  ]\n",
            " [150.     0.18   4.14  14.4   53.    23.43   7.71  44.  ]\n",
            " [136.     6.6    6.08  32.74  64.    33.28   2.72  49.  ]\n",
            " [138.     8.8    3.12  22.41  63.    23.33 120.03  55.  ]\n",
            " [144.     0.4    4.64  30.09  30.    27.39   0.74  55.  ]\n",
            " [102.     0.4    3.41  17.22  56.    23.59   2.06  39.  ]\n",
            " [154.     4.2    5.59  25.02  58.    25.02   1.54  43.  ]\n",
            " [128.     0.42   4.6   26.68  41.    30.97  10.33  31.  ]\n",
            " [128.     0.73   3.97  23.52  54.    23.81  19.2   64.  ]\n",
            " [162.     7.4    8.55  24.65  64.    25.71   5.86  58.  ]\n",
            " [132.     0.     6.63  29.58  37.    29.41   2.57  62.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_71G7vBnGY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, label_batch = next(iter(packed_train_data))\n",
        "test_batch, label_batch = next(iter(packed_test_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdJQB0xbnRCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "e99ba1be-3b9b-4901-d7ed-929368a82d64"
      },
      "source": [
        "import pandas as pd\n",
        "desc = pd.read_csv(train_file_path)[NUMERIC_FEATURES].describe()\n",
        "desc"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sbp</th>\n",
              "      <th>tobacco</th>\n",
              "      <th>ldl</th>\n",
              "      <th>adiposity</th>\n",
              "      <th>typea</th>\n",
              "      <th>obesity</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "      <td>395.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>137.207595</td>\n",
              "      <td>3.650810</td>\n",
              "      <td>4.776937</td>\n",
              "      <td>25.414405</td>\n",
              "      <td>53.465823</td>\n",
              "      <td>26.137797</td>\n",
              "      <td>16.813468</td>\n",
              "      <td>43.179747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>18.988306</td>\n",
              "      <td>4.485022</td>\n",
              "      <td>2.024007</td>\n",
              "      <td>7.663108</td>\n",
              "      <td>9.906196</td>\n",
              "      <td>4.231244</td>\n",
              "      <td>24.434489</td>\n",
              "      <td>14.408669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>101.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>7.120000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>17.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>124.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>3.275000</td>\n",
              "      <td>19.860000</td>\n",
              "      <td>47.000000</td>\n",
              "      <td>23.180000</td>\n",
              "      <td>0.510000</td>\n",
              "      <td>32.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>134.000000</td>\n",
              "      <td>2.020000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>26.170000</td>\n",
              "      <td>54.000000</td>\n",
              "      <td>25.810000</td>\n",
              "      <td>7.710000</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>148.000000</td>\n",
              "      <td>5.500000</td>\n",
              "      <td>5.900000</td>\n",
              "      <td>31.135000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>28.450000</td>\n",
              "      <td>23.310000</td>\n",
              "      <td>56.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>208.000000</td>\n",
              "      <td>31.200000</td>\n",
              "      <td>15.330000</td>\n",
              "      <td>42.490000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>46.580000</td>\n",
              "      <td>147.190000</td>\n",
              "      <td>64.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              sbp     tobacco         ldl  ...     obesity     alcohol         age\n",
              "count  395.000000  395.000000  395.000000  ...  395.000000  395.000000  395.000000\n",
              "mean   137.207595    3.650810    4.776937  ...   26.137797   16.813468   43.179747\n",
              "std     18.988306    4.485022    2.024007  ...    4.231244   24.434489   14.408669\n",
              "min    101.000000    0.000000    0.980000  ...   17.750000    0.000000   15.000000\n",
              "25%    124.000000    0.100000    3.275000  ...   23.180000    0.510000   32.000000\n",
              "50%    134.000000    2.020000    4.410000  ...   25.810000    7.710000   45.000000\n",
              "75%    148.000000    5.500000    5.900000  ...   28.450000   23.310000   56.000000\n",
              "max    208.000000   31.200000   15.330000  ...   46.580000  147.190000   64.000000\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKerz5xcnTFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MEAN = np.array(desc.T['mean'])\n",
        "STD = np.array(desc.T['std'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcrr5pHUnUk6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_numeric_data(data, mean, std):\n",
        "  # Center the data\n",
        "  return (data-mean)/std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTxSK6UOnW4a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "47e88031-6943-48fc-a93d-dc98da3bfc29"
      },
      "source": [
        "# See what you just created.\n",
        "normalizer = functools.partial(normalize_numeric_data, mean=MEAN, std=STD)\n",
        "\n",
        "numeric_column = tf.feature_column.numeric_column('numeric', normalizer_fn=normalizer, shape=[len(NUMERIC_FEATURES)])\n",
        "numeric_columns = [numeric_column]\n",
        "numeric_column"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NumericColumn(key='numeric', shape=(8,), default_value=None, dtype=tf.float32, normalizer_fn=functools.partial(<function normalize_numeric_data at 0x7fdfce96d8c8>, mean=array([137.208,   3.651,   4.777,  25.414,  53.466,  26.138,  16.813,\n",
              "        43.18 ]), std=array([18.988,  4.485,  2.024,  7.663,  9.906,  4.231, 24.434, 14.409])))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z0S2uiNnYZK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        },
        "outputId": "d087c546-7835-43d8-a555-f70d16e80744"
      },
      "source": [
        "train_batch['numeric']"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=266, shape=(50, 8), dtype=float32, numpy=\n",
              "array([[134.  ,   0.  ,   2.4 ,  21.11,  57.  ,  22.45,   1.37,  18.  ],\n",
              "       [132.  ,   2.  ,   2.7 ,  21.57,  50.  ,  27.95,   9.26,  37.  ],\n",
              "       [126.  ,   3.1 ,   2.01,  32.97,  56.  ,  28.63,  26.74,  45.  ],\n",
              "       [102.  ,   0.4 ,   3.41,  17.22,  56.  ,  23.59,   2.06,  39.  ],\n",
              "       [164.  ,   5.6 ,   3.17,  30.98,  44.  ,  25.99,  43.2 ,  53.  ],\n",
              "       [148.  ,  15.  ,   4.98,  36.94,  72.  ,  31.83,  66.27,  41.  ],\n",
              "       [122.  ,   1.7 ,   5.28,  32.23,  51.  ,  24.08,   0.  ,  54.  ],\n",
              "       [162.  ,   6.94,   4.55,  33.36,  52.  ,  27.09,  32.06,  43.  ],\n",
              "       [112.  ,   9.65,   2.29,  17.2 ,  54.  ,  23.53,   0.68,  53.  ],\n",
              "       [152.  ,  12.18,   4.04,  37.83,  63.  ,  34.57,   4.17,  64.  ],\n",
              "       [144.  ,   6.75,   5.45,  29.81,  53.  ,  25.62,  26.23,  43.  ],\n",
              "       [120.  ,   7.5 ,  15.33,  22.  ,  60.  ,  25.31,  34.49,  49.  ],\n",
              "       [118.  ,   0.  ,   3.67,  12.13,  51.  ,  19.15,   0.6 ,  15.  ],\n",
              "       [160.  ,   3.  ,   9.19,  26.47,  39.  ,  28.25,  14.4 ,  54.  ],\n",
              "       [126.  ,   4.6 ,   7.4 ,  31.99,  57.  ,  28.67,   0.37,  60.  ],\n",
              "       [122.  ,   0.  ,   5.49,  19.56,  57.  ,  23.12,  14.02,  27.  ],\n",
              "       [134.  ,  13.6 ,   3.5 ,  27.78,  60.  ,  25.99,  57.34,  49.  ],\n",
              "       [122.  ,   0.  ,   3.76,  24.59,  56.  ,  24.36,   0.  ,  30.  ],\n",
              "       [130.  ,   1.75,   5.46,  34.34,  53.  ,  29.42,   0.  ,  58.  ],\n",
              "       [122.  ,   0.  ,   3.08,  16.3 ,  43.  ,  22.13,   0.  ,  16.  ],\n",
              "       [148.  ,   0.5 ,   5.97,  32.88,  54.  ,  29.27,   6.43,  42.  ],\n",
              "       [108.  ,   0.8 ,   2.47,  17.53,  47.  ,  22.18,   0.  ,  55.  ],\n",
              "       [136.  ,   5.8 ,   5.9 ,  27.55,  65.  ,  25.71,  14.4 ,  59.  ],\n",
              "       [128.  ,   0.4 ,   6.17,  26.35,  64.  ,  27.86,  11.11,  34.  ],\n",
              "       [124.  ,   6.  ,   5.21,  33.02,  64.  ,  29.37,   7.61,  58.  ],\n",
              "       [124.  ,   1.04,   2.84,  16.42,  46.  ,  20.17,   0.  ,  61.  ],\n",
              "       [148.  ,   8.2 ,   7.75,  34.46,  46.  ,  26.53,   6.04,  64.  ],\n",
              "       [130.  ,   0.05,   2.44,  28.25,  67.  ,  30.86,  40.32,  34.  ],\n",
              "       [154.  ,   0.31,   2.33,  16.48,  33.  ,  24.  ,  11.83,  17.  ],\n",
              "       [134.  ,   4.8 ,   6.58,  29.89,  55.  ,  24.73,  23.66,  63.  ],\n",
              "       [136.  ,   2.52,   3.95,  25.63,  51.  ,  21.86,   0.  ,  45.  ],\n",
              "       [114.  ,   3.6 ,   4.16,  22.58,  60.  ,  24.49,  65.31,  31.  ],\n",
              "       [128.  ,   0.  ,   3.22,  26.55,  39.  ,  26.59,  16.71,  49.  ],\n",
              "       [110.  ,   4.64,   4.55,  30.46,  48.  ,  30.9 ,  15.22,  46.  ],\n",
              "       [118.  ,   1.62,   9.01,  21.7 ,  59.  ,  25.89,  21.19,  40.  ],\n",
              "       [118.  ,   4.  ,   3.95,  18.96,  54.  ,  25.15,   8.33,  49.  ],\n",
              "       [166.  ,   0.07,   4.03,  29.29,  53.  ,  28.37,   0.  ,  27.  ],\n",
              "       [148.  ,   0.  ,   3.84,  17.26,  70.  ,  20.  ,   0.  ,  21.  ],\n",
              "       [160.  ,   1.52,   8.12,  29.3 ,  54.  ,  25.87,  12.86,  43.  ],\n",
              "       [152.  ,   0.9 ,   9.12,  30.23,  56.  ,  28.64,   0.37,  42.  ],\n",
              "       [132.  ,   7.28,   3.52,  12.33,  60.  ,  19.48,   2.06,  56.  ],\n",
              "       [130.  ,   2.61,   2.72,  22.99,  51.  ,  26.29,  13.37,  51.  ],\n",
              "       [142.  ,   2.4 ,   2.55,  23.89,  54.  ,  26.09,  59.14,  37.  ],\n",
              "       [150.  ,   0.3 ,   6.38,  33.99,  62.  ,  24.64,   0.  ,  50.  ],\n",
              "       [130.  ,   0.  ,   1.82,  10.45,  57.  ,  22.07,   2.06,  17.  ],\n",
              "       [152.  ,   1.68,   3.58,  25.43,  50.  ,  27.03,   0.  ,  32.  ],\n",
              "       [112.  ,   4.46,   7.18,  26.25,  69.  ,  27.29,   0.  ,  32.  ],\n",
              "       [162.  ,   1.5 ,   2.46,  19.39,  49.  ,  24.32,   0.  ,  59.  ],\n",
              "       [126.  ,   3.4 ,   4.87,  15.16,  65.  ,  22.01,  11.11,  38.  ],\n",
              "       [123.  ,   8.6 ,  11.17,  35.28,  70.  ,  33.14,   0.  ,  59.  ]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8CNlasinaDq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "outputId": "9e4e4d6e-7c8c-43a7-9276-4dc36a287de8"
      },
      "source": [
        "numeric_layer = tf.keras.layers.DenseFeatures(numeric_columns)\n",
        "numeric_layer(train_batch).numpy()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.169, -0.814, -1.174, -0.562,  0.357, -0.872, -0.632, -1.748],\n",
              "       [-0.274, -0.368, -1.026, -0.502, -0.35 ,  0.428, -0.309, -0.429],\n",
              "       [-0.59 , -0.123, -1.367,  0.986,  0.256,  0.589,  0.406,  0.126],\n",
              "       [-1.854, -0.725, -0.675, -1.069,  0.256, -0.602, -0.604, -0.29 ],\n",
              "       [ 1.411,  0.435, -0.794,  0.726, -0.956, -0.035,  1.08 ,  0.682],\n",
              "       [ 0.568,  2.53 ,  0.1  ,  1.504,  1.871,  1.345,  2.024, -0.151],\n",
              "       [-0.801, -0.435,  0.249,  0.889, -0.249, -0.486, -0.688,  0.751],\n",
              "       [ 1.306,  0.733, -0.112,  1.037, -0.148,  0.225,  0.624, -0.012],\n",
              "       [-1.328,  1.338, -1.229, -1.072,  0.054, -0.616, -0.66 ,  0.682],\n",
              "       [ 0.779,  1.902, -0.364,  1.62 ,  0.962,  1.993, -0.517,  1.445],\n",
              "       [ 0.358,  0.691,  0.333,  0.574, -0.047, -0.122,  0.385, -0.012],\n",
              "       [-0.906,  0.858,  5.214, -0.446,  0.66 , -0.196,  0.723,  0.404],\n",
              "       [-1.012, -0.814, -0.547, -1.734, -0.249, -1.651, -0.664, -1.956],\n",
              "       [ 1.2  , -0.145,  2.18 ,  0.138, -1.46 ,  0.499, -0.099,  0.751],\n",
              "       [-0.59 ,  0.212,  1.296,  0.858,  0.357,  0.598, -0.673,  1.167],\n",
              "       [-0.801, -0.814,  0.352, -0.764,  0.357, -0.713, -0.114, -1.123],\n",
              "       [-0.169,  2.218, -0.631,  0.309,  0.66 , -0.035,  1.659,  0.404],\n",
              "       [-0.801, -0.814, -0.502, -0.108,  0.256, -0.42 , -0.688, -0.915],\n",
              "       [-0.38 , -0.424,  0.337,  1.165, -0.047,  0.776, -0.688,  1.029],\n",
              "       [-0.801, -0.814, -0.838, -1.189, -1.056, -0.947, -0.688, -1.886],\n",
              "       [ 0.568, -0.703,  0.589,  0.974,  0.054,  0.74 , -0.425, -0.082],\n",
              "       [-1.538, -0.636, -1.14 , -1.029, -0.653, -0.935, -0.688,  0.82 ],\n",
              "       [-0.064,  0.479,  0.555,  0.279,  1.164, -0.101, -0.099,  1.098],\n",
              "       [-0.485, -0.725,  0.688,  0.122,  1.063,  0.407, -0.233, -0.637],\n",
              "       [-0.696,  0.524,  0.214,  0.992,  1.063,  0.764, -0.377,  1.029],\n",
              "       [-0.696, -0.582, -0.957, -1.174, -0.754, -1.41 , -0.688,  1.237],\n",
              "       [ 0.568,  1.014,  1.469,  1.18 , -0.754,  0.093, -0.441,  1.445],\n",
              "       [-0.38 , -0.803, -1.155,  0.37 ,  1.366,  1.116,  0.962, -0.637],\n",
              "       [ 0.884, -0.745, -1.209, -1.166, -2.066, -0.505, -0.204, -1.817],\n",
              "       [-0.169,  0.256,  0.891,  0.584,  0.155, -0.333,  0.28 ,  1.376],\n",
              "       [-0.064, -0.252, -0.409,  0.028, -0.249, -1.011, -0.688,  0.126],\n",
              "       [-1.222, -0.011, -0.305, -0.37 ,  0.66 , -0.389,  1.985, -0.845],\n",
              "       [-0.485, -0.814, -0.769,  0.148, -1.46 ,  0.107, -0.004,  0.404],\n",
              "       [-1.433,  0.221, -0.112,  0.658, -0.552,  1.125, -0.065,  0.196],\n",
              "       [-1.012, -0.453,  2.091, -0.485,  0.559, -0.059,  0.179, -0.221],\n",
              "       [-1.012,  0.078, -0.409, -0.842,  0.054, -0.233, -0.347,  0.404],\n",
              "       [ 1.516, -0.798, -0.369,  0.506, -0.047,  0.528, -0.688, -1.123],\n",
              "       [ 0.568, -0.814, -0.463, -1.064,  1.669, -1.451, -0.688, -1.539],\n",
              "       [ 1.2  , -0.475,  1.652,  0.507,  0.054, -0.063, -0.162, -0.012],\n",
              "       [ 0.779, -0.613,  2.146,  0.628,  0.256,  0.591, -0.673, -0.082],\n",
              "       [-0.274,  0.809, -0.621, -1.707,  0.66 , -1.573, -0.604,  0.89 ],\n",
              "       [-0.38 , -0.232, -1.016, -0.316, -0.249,  0.036, -0.141,  0.543],\n",
              "       [ 0.252, -0.279, -1.1  , -0.199,  0.054, -0.011,  1.732, -0.429],\n",
              "       [ 0.674, -0.747,  0.792,  1.119,  0.861, -0.354, -0.688,  0.473],\n",
              "       [-0.38 , -0.814, -1.461, -1.953,  0.357, -0.961, -0.604, -1.817],\n",
              "       [ 0.779, -0.439, -0.591,  0.002, -0.35 ,  0.211, -0.688, -0.776],\n",
              "       [-1.328,  0.18 ,  1.187,  0.109,  1.568,  0.272, -0.688, -0.776],\n",
              "       [ 1.306, -0.48 , -1.145, -0.786, -0.451, -0.43 , -0.688,  1.098],\n",
              "       [-0.59 , -0.056,  0.046, -1.338,  1.164, -0.976, -0.233, -0.359],\n",
              "       [-0.748,  1.103,  3.159,  1.287,  1.669,  1.655, -0.688,  1.098]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5piDlDBnfp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CATEGORIES = {\n",
        "    'famhist': ['Present', 'Absent']\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BAPQCwanhW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categorical_columns = []\n",
        "for feature, vocab in CATEGORIES.items():\n",
        "  cat_col = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "        key=feature, vocabulary_list=vocab)\n",
        "  categorical_columns.append(tf.feature_column.indicator_column(cat_col))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0LeTmsdnjUd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "df971b53-7a9c-477c-c830-599967200cf0"
      },
      "source": [
        "categorical_columns"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='famhist', vocabulary_list=('Present', 'Absent'), dtype=tf.string, default_value=-1, num_oov_buckets=0))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZ8CsUL8nlWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "8271c68d-99d0-4363-a295-1e715844d915"
      },
      "source": [
        "categorical_layer = tf.keras.layers.DenseFeatures(categorical_columns)\n",
        "print(categorical_layer(train_batch).numpy()[0])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "WARNING:tensorflow:From /tensorflow-2.0.0/python3.6/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMgyObENnm7m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessing_layer = tf.keras.layers.DenseFeatures(categorical_columns+numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg8w0j3fnoQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "43f0c9cc-4ad9-4f8c-be25-cde80a345b7e"
      },
      "source": [
        "print(preprocessing_layer(train_batch).numpy()[0])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.     1.    -0.169 -0.814 -1.174 -0.562  0.357 -0.872 -0.632 -1.748]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBbNGt-FnpvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  preprocessing_layer,\n",
        "  tf.keras.layers.Dense(128, kernel_regularizer=tf.keras.regularizers.l2(0.01), activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid'),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    optimizer='adamax',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggW-lHPTnr3Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = packed_train_data.shuffle(500)\n",
        "test_data = packed_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU9VUfp7ntQg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c7a870b-7c24-4d1d-e631-39d47d00320e"
      },
      "source": [
        "print(\"--Train Model--\")\n",
        "model.fit(train_data, epochs=500)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--Train Model--\n",
            "Epoch 1/500\n",
            "8/8 [==============================] - 1s 90ms/step - loss: 0.8486 - accuracy: 0.6076\n",
            "Epoch 2/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.7858 - accuracy: 0.6734\n",
            "Epoch 3/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.7466 - accuracy: 0.6734\n",
            "Epoch 4/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.7348 - accuracy: 0.7038\n",
            "Epoch 5/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.7054 - accuracy: 0.7038\n",
            "Epoch 6/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.7018 - accuracy: 0.7215\n",
            "Epoch 7/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6968 - accuracy: 0.7316\n",
            "Epoch 8/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6659 - accuracy: 0.7291\n",
            "Epoch 9/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6600 - accuracy: 0.7468\n",
            "Epoch 10/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6520 - accuracy: 0.7544\n",
            "Epoch 11/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6459 - accuracy: 0.7418\n",
            "Epoch 12/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.6476 - accuracy: 0.7418\n",
            "Epoch 13/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6259 - accuracy: 0.7519\n",
            "Epoch 14/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6458 - accuracy: 0.7494\n",
            "Epoch 15/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6284 - accuracy: 0.7519\n",
            "Epoch 16/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.6235 - accuracy: 0.7519\n",
            "Epoch 17/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6235 - accuracy: 0.7468\n",
            "Epoch 18/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5977 - accuracy: 0.7620\n",
            "Epoch 19/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.7696\n",
            "Epoch 20/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6007 - accuracy: 0.7696\n",
            "Epoch 21/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5879 - accuracy: 0.7544\n",
            "Epoch 22/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5897 - accuracy: 0.7620\n",
            "Epoch 23/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.6073 - accuracy: 0.7595\n",
            "Epoch 24/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5792 - accuracy: 0.7671\n",
            "Epoch 25/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5852 - accuracy: 0.7595\n",
            "Epoch 26/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5742 - accuracy: 0.7671\n",
            "Epoch 27/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5770 - accuracy: 0.7747\n",
            "Epoch 28/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5797 - accuracy: 0.7797\n",
            "Epoch 29/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5804 - accuracy: 0.7848\n",
            "Epoch 30/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5526 - accuracy: 0.7772\n",
            "Epoch 31/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5527 - accuracy: 0.7924\n",
            "Epoch 32/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5607 - accuracy: 0.7823\n",
            "Epoch 33/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5595 - accuracy: 0.7873\n",
            "Epoch 34/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5510 - accuracy: 0.7772\n",
            "Epoch 35/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5495 - accuracy: 0.7772\n",
            "Epoch 36/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5314 - accuracy: 0.7848\n",
            "Epoch 37/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5439 - accuracy: 0.7747\n",
            "Epoch 38/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5608 - accuracy: 0.7544\n",
            "Epoch 39/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5335 - accuracy: 0.7873\n",
            "Epoch 40/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5292 - accuracy: 0.7924\n",
            "Epoch 41/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5277 - accuracy: 0.7975\n",
            "Epoch 42/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.5234 - accuracy: 0.8051\n",
            "Epoch 43/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5288 - accuracy: 0.7823\n",
            "Epoch 44/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5158 - accuracy: 0.8025\n",
            "Epoch 45/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5517 - accuracy: 0.7949\n",
            "Epoch 46/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.8025\n",
            "Epoch 47/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5243 - accuracy: 0.7873\n",
            "Epoch 48/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5090 - accuracy: 0.8025\n",
            "Epoch 49/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5073 - accuracy: 0.8152\n",
            "Epoch 50/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.5069 - accuracy: 0.8000\n",
            "Epoch 51/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5170 - accuracy: 0.7848\n",
            "Epoch 52/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5164 - accuracy: 0.8101\n",
            "Epoch 53/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5048 - accuracy: 0.8152\n",
            "Epoch 54/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4979 - accuracy: 0.8253\n",
            "Epoch 55/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5082 - accuracy: 0.8177\n",
            "Epoch 56/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4954 - accuracy: 0.8228\n",
            "Epoch 57/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.5036 - accuracy: 0.8203\n",
            "Epoch 58/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4769 - accuracy: 0.8278\n",
            "Epoch 59/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4849 - accuracy: 0.8203\n",
            "Epoch 60/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4815 - accuracy: 0.8152\n",
            "Epoch 61/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4704 - accuracy: 0.8177\n",
            "Epoch 62/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4791 - accuracy: 0.8329\n",
            "Epoch 63/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4843 - accuracy: 0.8456\n",
            "Epoch 64/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4748 - accuracy: 0.8329\n",
            "Epoch 65/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4622 - accuracy: 0.8304\n",
            "Epoch 66/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4627 - accuracy: 0.8253\n",
            "Epoch 67/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4693 - accuracy: 0.8228\n",
            "Epoch 68/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4749 - accuracy: 0.8329\n",
            "Epoch 69/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4779 - accuracy: 0.8253\n",
            "Epoch 70/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4677 - accuracy: 0.8354\n",
            "Epoch 71/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4320 - accuracy: 0.8557\n",
            "Epoch 72/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4648 - accuracy: 0.8253\n",
            "Epoch 73/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.4371 - accuracy: 0.8506\n",
            "Epoch 74/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4593 - accuracy: 0.8329\n",
            "Epoch 75/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4451 - accuracy: 0.8430\n",
            "Epoch 76/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4575 - accuracy: 0.8329\n",
            "Epoch 77/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4386 - accuracy: 0.8405\n",
            "Epoch 78/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4560 - accuracy: 0.8228\n",
            "Epoch 79/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4271 - accuracy: 0.8658\n",
            "Epoch 80/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4360 - accuracy: 0.8430\n",
            "Epoch 81/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4551 - accuracy: 0.8354\n",
            "Epoch 82/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4419 - accuracy: 0.8481\n",
            "Epoch 83/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4309 - accuracy: 0.8582\n",
            "Epoch 84/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4333 - accuracy: 0.8506\n",
            "Epoch 85/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4273 - accuracy: 0.8532\n",
            "Epoch 86/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.4174 - accuracy: 0.8658\n",
            "Epoch 87/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4319 - accuracy: 0.8430\n",
            "Epoch 88/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4345 - accuracy: 0.8380\n",
            "Epoch 89/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4285 - accuracy: 0.8582\n",
            "Epoch 90/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4148 - accuracy: 0.8405\n",
            "Epoch 91/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3964 - accuracy: 0.8430\n",
            "Epoch 92/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4033 - accuracy: 0.8810\n",
            "Epoch 93/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4152 - accuracy: 0.8557\n",
            "Epoch 94/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3864 - accuracy: 0.8633\n",
            "Epoch 95/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4136 - accuracy: 0.8557\n",
            "Epoch 96/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3947 - accuracy: 0.8684\n",
            "Epoch 97/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4021 - accuracy: 0.8506\n",
            "Epoch 98/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3895 - accuracy: 0.8835\n",
            "Epoch 99/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3940 - accuracy: 0.8684\n",
            "Epoch 100/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.4051 - accuracy: 0.8557\n",
            "Epoch 101/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3913 - accuracy: 0.8684\n",
            "Epoch 102/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3950 - accuracy: 0.8684\n",
            "Epoch 103/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3848 - accuracy: 0.8734\n",
            "Epoch 104/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3846 - accuracy: 0.8684\n",
            "Epoch 105/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3816 - accuracy: 0.8734\n",
            "Epoch 106/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3904 - accuracy: 0.8684\n",
            "Epoch 107/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3905 - accuracy: 0.8759\n",
            "Epoch 108/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3663 - accuracy: 0.8658\n",
            "Epoch 109/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3642 - accuracy: 0.8759\n",
            "Epoch 110/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3635 - accuracy: 0.8886\n",
            "Epoch 111/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3741 - accuracy: 0.8709\n",
            "Epoch 112/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3740 - accuracy: 0.8658\n",
            "Epoch 113/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3640 - accuracy: 0.8759\n",
            "Epoch 114/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3815 - accuracy: 0.8810\n",
            "Epoch 115/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3632 - accuracy: 0.8785\n",
            "Epoch 116/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3919 - accuracy: 0.8734\n",
            "Epoch 117/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3769 - accuracy: 0.8734\n",
            "Epoch 118/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3506 - accuracy: 0.8810\n",
            "Epoch 119/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3312 - accuracy: 0.8759\n",
            "Epoch 120/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3528 - accuracy: 0.8911\n",
            "Epoch 121/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3556 - accuracy: 0.8633\n",
            "Epoch 122/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3486 - accuracy: 0.8886\n",
            "Epoch 123/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3523 - accuracy: 0.8785\n",
            "Epoch 124/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3378 - accuracy: 0.8937\n",
            "Epoch 125/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3328 - accuracy: 0.8987\n",
            "Epoch 126/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3393 - accuracy: 0.8886\n",
            "Epoch 127/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3654 - accuracy: 0.8633\n",
            "Epoch 128/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3487 - accuracy: 0.8734\n",
            "Epoch 129/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3601 - accuracy: 0.8709\n",
            "Epoch 130/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3527 - accuracy: 0.8684\n",
            "Epoch 131/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3279 - accuracy: 0.8987\n",
            "Epoch 132/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3509 - accuracy: 0.8835\n",
            "Epoch 133/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3104 - accuracy: 0.8886\n",
            "Epoch 134/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3457 - accuracy: 0.8861\n",
            "Epoch 135/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3273 - accuracy: 0.9013\n",
            "Epoch 136/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3271 - accuracy: 0.8911\n",
            "Epoch 137/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3346 - accuracy: 0.8861\n",
            "Epoch 138/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3151 - accuracy: 0.8911\n",
            "Epoch 139/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3271 - accuracy: 0.9013\n",
            "Epoch 140/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3118 - accuracy: 0.8962\n",
            "Epoch 141/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3107 - accuracy: 0.9013\n",
            "Epoch 142/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3388 - accuracy: 0.9013\n",
            "Epoch 143/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3058 - accuracy: 0.8886\n",
            "Epoch 144/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3119 - accuracy: 0.9114\n",
            "Epoch 145/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3138 - accuracy: 0.8886\n",
            "Epoch 146/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.3023 - accuracy: 0.9241\n",
            "Epoch 147/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3035 - accuracy: 0.8962\n",
            "Epoch 148/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3106 - accuracy: 0.9013\n",
            "Epoch 149/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.3102 - accuracy: 0.8785\n",
            "Epoch 150/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3204 - accuracy: 0.9038\n",
            "Epoch 151/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2930 - accuracy: 0.9038\n",
            "Epoch 152/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3099 - accuracy: 0.8911\n",
            "Epoch 153/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2899 - accuracy: 0.9013\n",
            "Epoch 154/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3233 - accuracy: 0.8911\n",
            "Epoch 155/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2972 - accuracy: 0.9139\n",
            "Epoch 156/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2806 - accuracy: 0.9215\n",
            "Epoch 157/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3076 - accuracy: 0.8886\n",
            "Epoch 158/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2885 - accuracy: 0.9063\n",
            "Epoch 159/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3048 - accuracy: 0.9013\n",
            "Epoch 160/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3099 - accuracy: 0.9063\n",
            "Epoch 161/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2724 - accuracy: 0.9215\n",
            "Epoch 162/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2777 - accuracy: 0.9139\n",
            "Epoch 163/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2845 - accuracy: 0.9038\n",
            "Epoch 164/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.3009 - accuracy: 0.9190\n",
            "Epoch 165/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2885 - accuracy: 0.9139\n",
            "Epoch 166/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2691 - accuracy: 0.9190\n",
            "Epoch 167/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2942 - accuracy: 0.9063\n",
            "Epoch 168/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2750 - accuracy: 0.9190\n",
            "Epoch 169/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2879 - accuracy: 0.8987\n",
            "Epoch 170/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.3017 - accuracy: 0.9063\n",
            "Epoch 171/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2701 - accuracy: 0.9165\n",
            "Epoch 172/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2608 - accuracy: 0.9342\n",
            "Epoch 173/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.9367\n",
            "Epoch 174/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2718 - accuracy: 0.9266\n",
            "Epoch 175/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2630 - accuracy: 0.9241\n",
            "Epoch 176/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2691 - accuracy: 0.9114\n",
            "Epoch 177/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2468 - accuracy: 0.9190\n",
            "Epoch 178/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2504 - accuracy: 0.9367\n",
            "Epoch 179/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2659 - accuracy: 0.9089\n",
            "Epoch 180/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2569 - accuracy: 0.9266\n",
            "Epoch 181/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2666 - accuracy: 0.9215\n",
            "Epoch 182/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2497 - accuracy: 0.9316\n",
            "Epoch 183/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2663 - accuracy: 0.9266\n",
            "Epoch 184/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2599 - accuracy: 0.9165\n",
            "Epoch 185/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2342 - accuracy: 0.9215\n",
            "Epoch 186/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2602 - accuracy: 0.9215\n",
            "Epoch 187/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2415 - accuracy: 0.9266\n",
            "Epoch 188/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2512 - accuracy: 0.9367\n",
            "Epoch 189/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2402 - accuracy: 0.9443\n",
            "Epoch 190/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2886 - accuracy: 0.9139\n",
            "Epoch 191/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2467 - accuracy: 0.9494\n",
            "Epoch 192/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2399 - accuracy: 0.9291\n",
            "Epoch 193/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2324 - accuracy: 0.9266\n",
            "Epoch 194/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2449 - accuracy: 0.9418\n",
            "Epoch 195/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2563 - accuracy: 0.9316\n",
            "Epoch 196/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2390 - accuracy: 0.9443\n",
            "Epoch 197/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2253 - accuracy: 0.9519\n",
            "Epoch 198/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2318 - accuracy: 0.9316\n",
            "Epoch 199/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2398 - accuracy: 0.9291\n",
            "Epoch 200/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2414 - accuracy: 0.9418\n",
            "Epoch 201/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2557 - accuracy: 0.9266\n",
            "Epoch 202/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2375 - accuracy: 0.9342\n",
            "Epoch 203/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.2227 - accuracy: 0.9266\n",
            "Epoch 204/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2220 - accuracy: 0.9443\n",
            "Epoch 205/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2343 - accuracy: 0.9418\n",
            "Epoch 206/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2309 - accuracy: 0.9316\n",
            "Epoch 207/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2216 - accuracy: 0.9392\n",
            "Epoch 208/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2277 - accuracy: 0.9316\n",
            "Epoch 209/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2374 - accuracy: 0.9367\n",
            "Epoch 210/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2329 - accuracy: 0.9367\n",
            "Epoch 211/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2041 - accuracy: 0.9494\n",
            "Epoch 212/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2282 - accuracy: 0.9291\n",
            "Epoch 213/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2505 - accuracy: 0.9241\n",
            "Epoch 214/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2467 - accuracy: 0.9165\n",
            "Epoch 215/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2108 - accuracy: 0.9544\n",
            "Epoch 216/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2349 - accuracy: 0.9367\n",
            "Epoch 217/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2158 - accuracy: 0.9468\n",
            "Epoch 218/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2211 - accuracy: 0.9367\n",
            "Epoch 219/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2201 - accuracy: 0.9392\n",
            "Epoch 220/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2324 - accuracy: 0.9241\n",
            "Epoch 221/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2200 - accuracy: 0.9494\n",
            "Epoch 222/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2099 - accuracy: 0.9519\n",
            "Epoch 223/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2336 - accuracy: 0.9418\n",
            "Epoch 224/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2117 - accuracy: 0.9418\n",
            "Epoch 225/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2177 - accuracy: 0.9392\n",
            "Epoch 226/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2092 - accuracy: 0.9494\n",
            "Epoch 227/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2097 - accuracy: 0.9418\n",
            "Epoch 228/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2002 - accuracy: 0.9544\n",
            "Epoch 229/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1926 - accuracy: 0.9494\n",
            "Epoch 230/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2330 - accuracy: 0.9342\n",
            "Epoch 231/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2035 - accuracy: 0.9570\n",
            "Epoch 232/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.2025 - accuracy: 0.9468\n",
            "Epoch 233/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2072 - accuracy: 0.9519\n",
            "Epoch 234/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2015 - accuracy: 0.9544\n",
            "Epoch 235/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1804 - accuracy: 0.9595\n",
            "Epoch 236/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1914 - accuracy: 0.9696\n",
            "Epoch 237/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1994 - accuracy: 0.9443\n",
            "Epoch 238/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1980 - accuracy: 0.9494\n",
            "Epoch 239/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1966 - accuracy: 0.9570\n",
            "Epoch 240/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1903 - accuracy: 0.9468\n",
            "Epoch 241/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1952 - accuracy: 0.9392\n",
            "Epoch 242/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1921 - accuracy: 0.9620\n",
            "Epoch 243/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.2008 - accuracy: 0.9519\n",
            "Epoch 244/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1924 - accuracy: 0.9494\n",
            "Epoch 245/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2094 - accuracy: 0.9519\n",
            "Epoch 246/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1773 - accuracy: 0.9620\n",
            "Epoch 247/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1916 - accuracy: 0.9595\n",
            "Epoch 248/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1866 - accuracy: 0.9620\n",
            "Epoch 249/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1904 - accuracy: 0.9671\n",
            "Epoch 250/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1856 - accuracy: 0.9595\n",
            "Epoch 251/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1976 - accuracy: 0.9418\n",
            "Epoch 252/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1746 - accuracy: 0.9620\n",
            "Epoch 253/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1767 - accuracy: 0.9570\n",
            "Epoch 254/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1882 - accuracy: 0.9418\n",
            "Epoch 255/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1692 - accuracy: 0.9570\n",
            "Epoch 256/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1787 - accuracy: 0.9595\n",
            "Epoch 257/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1738 - accuracy: 0.9570\n",
            "Epoch 258/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1893 - accuracy: 0.9468\n",
            "Epoch 259/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1726 - accuracy: 0.9696\n",
            "Epoch 260/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1750 - accuracy: 0.9519\n",
            "Epoch 261/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1741 - accuracy: 0.9646\n",
            "Epoch 262/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1797 - accuracy: 0.9671\n",
            "Epoch 263/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2118 - accuracy: 0.9316\n",
            "Epoch 264/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1950 - accuracy: 0.9494\n",
            "Epoch 265/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1790 - accuracy: 0.9696\n",
            "Epoch 266/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1625 - accuracy: 0.9671\n",
            "Epoch 267/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1656 - accuracy: 0.9696\n",
            "Epoch 268/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1673 - accuracy: 0.9646\n",
            "Epoch 269/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1684 - accuracy: 0.9595\n",
            "Epoch 270/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1679 - accuracy: 0.9646\n",
            "Epoch 271/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1705 - accuracy: 0.9620\n",
            "Epoch 272/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1878 - accuracy: 0.9544\n",
            "Epoch 273/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1668 - accuracy: 0.9494\n",
            "Epoch 274/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1719 - accuracy: 0.9519\n",
            "Epoch 275/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1635 - accuracy: 0.9620\n",
            "Epoch 276/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1640 - accuracy: 0.9671\n",
            "Epoch 277/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1595 - accuracy: 0.9722\n",
            "Epoch 278/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1856 - accuracy: 0.9570\n",
            "Epoch 279/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1510 - accuracy: 0.9747\n",
            "Epoch 280/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1609 - accuracy: 0.9595\n",
            "Epoch 281/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1811 - accuracy: 0.9620\n",
            "Epoch 282/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1745 - accuracy: 0.9544\n",
            "Epoch 283/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1465 - accuracy: 0.9646\n",
            "Epoch 284/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1486 - accuracy: 0.9873\n",
            "Epoch 285/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1814 - accuracy: 0.9570\n",
            "Epoch 286/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9747\n",
            "Epoch 287/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1733 - accuracy: 0.9570\n",
            "Epoch 288/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1374 - accuracy: 0.9797\n",
            "Epoch 289/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1733 - accuracy: 0.9494\n",
            "Epoch 290/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1683 - accuracy: 0.9671\n",
            "Epoch 291/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1639 - accuracy: 0.9494\n",
            "Epoch 292/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1672 - accuracy: 0.9797\n",
            "Epoch 293/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1749 - accuracy: 0.9696\n",
            "Epoch 294/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1445 - accuracy: 0.9772\n",
            "Epoch 295/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1573 - accuracy: 0.9696\n",
            "Epoch 296/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1427 - accuracy: 0.9772\n",
            "Epoch 297/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1687 - accuracy: 0.9671\n",
            "Epoch 298/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1282 - accuracy: 0.9848\n",
            "Epoch 299/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1553 - accuracy: 0.9646\n",
            "Epoch 300/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1626 - accuracy: 0.9570\n",
            "Epoch 301/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1483 - accuracy: 0.9696\n",
            "Epoch 302/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9747\n",
            "Epoch 303/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1554 - accuracy: 0.9544\n",
            "Epoch 304/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.2273 - accuracy: 0.9443\n",
            "Epoch 305/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1399 - accuracy: 0.9747\n",
            "Epoch 306/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1590 - accuracy: 0.9620\n",
            "Epoch 307/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9772\n",
            "Epoch 308/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1578 - accuracy: 0.9595\n",
            "Epoch 309/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1335 - accuracy: 0.9797\n",
            "Epoch 310/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1594 - accuracy: 0.9595\n",
            "Epoch 311/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1734 - accuracy: 0.9620\n",
            "Epoch 312/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1642 - accuracy: 0.9519\n",
            "Epoch 313/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1561 - accuracy: 0.9722\n",
            "Epoch 314/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1507 - accuracy: 0.9646\n",
            "Epoch 315/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1687 - accuracy: 0.9570\n",
            "Epoch 316/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1396 - accuracy: 0.9722\n",
            "Epoch 317/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1461 - accuracy: 0.9595\n",
            "Epoch 318/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1690 - accuracy: 0.9443\n",
            "Epoch 319/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1419 - accuracy: 0.9620\n",
            "Epoch 320/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1548 - accuracy: 0.9646\n",
            "Epoch 321/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1512 - accuracy: 0.9696\n",
            "Epoch 322/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1250 - accuracy: 0.9747\n",
            "Epoch 323/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1460 - accuracy: 0.9722\n",
            "Epoch 324/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1517 - accuracy: 0.9747\n",
            "Epoch 325/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1518 - accuracy: 0.9696\n",
            "Epoch 326/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1527 - accuracy: 0.9570\n",
            "Epoch 327/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9797\n",
            "Epoch 328/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1485 - accuracy: 0.9722\n",
            "Epoch 329/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1263 - accuracy: 0.9747\n",
            "Epoch 330/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9646\n",
            "Epoch 331/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1589 - accuracy: 0.9722\n",
            "Epoch 332/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1514 - accuracy: 0.9696\n",
            "Epoch 333/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1402 - accuracy: 0.9772\n",
            "Epoch 334/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1398 - accuracy: 0.9848\n",
            "Epoch 335/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1533 - accuracy: 0.9620\n",
            "Epoch 336/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1528 - accuracy: 0.9671\n",
            "Epoch 337/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9747\n",
            "Epoch 338/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1392 - accuracy: 0.9722\n",
            "Epoch 339/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1541 - accuracy: 0.9620\n",
            "Epoch 340/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1679 - accuracy: 0.9646\n",
            "Epoch 341/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1473 - accuracy: 0.9747\n",
            "Epoch 342/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1344 - accuracy: 0.9696\n",
            "Epoch 343/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1198 - accuracy: 0.9848\n",
            "Epoch 344/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1562 - accuracy: 0.9696\n",
            "Epoch 345/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1376 - accuracy: 0.9722\n",
            "Epoch 346/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1598 - accuracy: 0.9646\n",
            "Epoch 347/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1373 - accuracy: 0.9747\n",
            "Epoch 348/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1204 - accuracy: 0.9797\n",
            "Epoch 349/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1258 - accuracy: 0.9823\n",
            "Epoch 350/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1180 - accuracy: 0.9797\n",
            "Epoch 351/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1208 - accuracy: 0.9747\n",
            "Epoch 352/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1182 - accuracy: 0.9848\n",
            "Epoch 353/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1406 - accuracy: 0.9747\n",
            "Epoch 354/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1319 - accuracy: 0.9823\n",
            "Epoch 355/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1631 - accuracy: 0.9595\n",
            "Epoch 356/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1336 - accuracy: 0.9823\n",
            "Epoch 357/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1355 - accuracy: 0.9722\n",
            "Epoch 358/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1406 - accuracy: 0.9797\n",
            "Epoch 359/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1524 - accuracy: 0.9696\n",
            "Epoch 360/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1376 - accuracy: 0.9722\n",
            "Epoch 361/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1375 - accuracy: 0.9696\n",
            "Epoch 362/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1348 - accuracy: 0.9671\n",
            "Epoch 363/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1239 - accuracy: 0.9848\n",
            "Epoch 364/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1105 - accuracy: 0.9873\n",
            "Epoch 365/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1195 - accuracy: 0.9747\n",
            "Epoch 366/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1276 - accuracy: 0.9772\n",
            "Epoch 367/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1306 - accuracy: 0.9772\n",
            "Epoch 368/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1430 - accuracy: 0.9696\n",
            "Epoch 369/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1178 - accuracy: 0.9747\n",
            "Epoch 370/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1416 - accuracy: 0.9620\n",
            "Epoch 371/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1331 - accuracy: 0.9747\n",
            "Epoch 372/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1298 - accuracy: 0.9747\n",
            "Epoch 373/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1371 - accuracy: 0.9722\n",
            "Epoch 374/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1296 - accuracy: 0.9747\n",
            "Epoch 375/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1309 - accuracy: 0.9772\n",
            "Epoch 376/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1244 - accuracy: 0.9772\n",
            "Epoch 377/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1195 - accuracy: 0.9823\n",
            "Epoch 378/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1264 - accuracy: 0.9747\n",
            "Epoch 379/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1320 - accuracy: 0.9696\n",
            "Epoch 380/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1159 - accuracy: 0.9873\n",
            "Epoch 381/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1244 - accuracy: 0.9722\n",
            "Epoch 382/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1186 - accuracy: 0.9823\n",
            "Epoch 383/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1269 - accuracy: 0.9797\n",
            "Epoch 384/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1076 - accuracy: 0.9873\n",
            "Epoch 385/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1151 - accuracy: 0.9873\n",
            "Epoch 386/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1181 - accuracy: 0.9772\n",
            "Epoch 387/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1128 - accuracy: 0.9823\n",
            "Epoch 388/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1272 - accuracy: 0.9772\n",
            "Epoch 389/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1395 - accuracy: 0.9722\n",
            "Epoch 390/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1019 - accuracy: 0.9899\n",
            "Epoch 391/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1202 - accuracy: 0.9797\n",
            "Epoch 392/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1020 - accuracy: 0.9899\n",
            "Epoch 393/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9823\n",
            "Epoch 394/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1121 - accuracy: 0.9823\n",
            "Epoch 395/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1175 - accuracy: 0.9772\n",
            "Epoch 396/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1275 - accuracy: 0.9797\n",
            "Epoch 397/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1087 - accuracy: 0.9797\n",
            "Epoch 398/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1177 - accuracy: 0.9747\n",
            "Epoch 399/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1165 - accuracy: 0.9797\n",
            "Epoch 400/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1151 - accuracy: 0.9848\n",
            "Epoch 401/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1118 - accuracy: 0.9848\n",
            "Epoch 402/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1406 - accuracy: 0.9722\n",
            "Epoch 403/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1212 - accuracy: 0.9772\n",
            "Epoch 404/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1204 - accuracy: 0.9671\n",
            "Epoch 405/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1445 - accuracy: 0.9722\n",
            "Epoch 406/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1177 - accuracy: 0.9873\n",
            "Epoch 407/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1247 - accuracy: 0.9797\n",
            "Epoch 408/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1040 - accuracy: 0.9924\n",
            "Epoch 409/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1186 - accuracy: 0.9772\n",
            "Epoch 410/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1020 - accuracy: 0.9848\n",
            "Epoch 411/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1229 - accuracy: 0.9722\n",
            "Epoch 412/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0995 - accuracy: 0.9873\n",
            "Epoch 413/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1125 - accuracy: 0.9848\n",
            "Epoch 414/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1035 - accuracy: 0.9899\n",
            "Epoch 415/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1372 - accuracy: 0.9722\n",
            "Epoch 416/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1094 - accuracy: 0.9797\n",
            "Epoch 417/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1240 - accuracy: 0.9772\n",
            "Epoch 418/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1153 - accuracy: 0.9696\n",
            "Epoch 419/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1249 - accuracy: 0.9747\n",
            "Epoch 420/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1047 - accuracy: 0.9823\n",
            "Epoch 421/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1064 - accuracy: 0.9823\n",
            "Epoch 422/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0993 - accuracy: 0.9848\n",
            "Epoch 423/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1176 - accuracy: 0.9823\n",
            "Epoch 424/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1048 - accuracy: 0.9848\n",
            "Epoch 425/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1311 - accuracy: 0.9696\n",
            "Epoch 426/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1216 - accuracy: 0.9772\n",
            "Epoch 427/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1277 - accuracy: 0.9696\n",
            "Epoch 428/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1129 - accuracy: 0.9747\n",
            "Epoch 429/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1188 - accuracy: 0.9848\n",
            "Epoch 430/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1106 - accuracy: 0.9873\n",
            "Epoch 431/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0988 - accuracy: 0.9924\n",
            "Epoch 432/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0965 - accuracy: 0.9873\n",
            "Epoch 433/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1048 - accuracy: 0.9797\n",
            "Epoch 434/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1019 - accuracy: 0.9823\n",
            "Epoch 435/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1104 - accuracy: 0.9772\n",
            "Epoch 436/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1197 - accuracy: 0.9722\n",
            "Epoch 437/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1064 - accuracy: 0.9823\n",
            "Epoch 438/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1376 - accuracy: 0.9671\n",
            "Epoch 439/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1078 - accuracy: 0.9696\n",
            "Epoch 440/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1236 - accuracy: 0.9747\n",
            "Epoch 441/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1084 - accuracy: 0.9899\n",
            "Epoch 442/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1105 - accuracy: 0.9772\n",
            "Epoch 443/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1080 - accuracy: 0.9772\n",
            "Epoch 444/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1150 - accuracy: 0.9797\n",
            "Epoch 445/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1102 - accuracy: 0.9747\n",
            "Epoch 446/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1112 - accuracy: 0.9823\n",
            "Epoch 447/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1097 - accuracy: 0.9797\n",
            "Epoch 448/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1144 - accuracy: 0.9797\n",
            "Epoch 449/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1093 - accuracy: 0.9797\n",
            "Epoch 450/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1026 - accuracy: 0.9772\n",
            "Epoch 451/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1097 - accuracy: 0.9797\n",
            "Epoch 452/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0988 - accuracy: 0.9899\n",
            "Epoch 453/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1153 - accuracy: 0.9873\n",
            "Epoch 454/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1145 - accuracy: 0.9848\n",
            "Epoch 455/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0890 - accuracy: 0.9873\n",
            "Epoch 456/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1097 - accuracy: 0.9772\n",
            "Epoch 457/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1013 - accuracy: 0.9873\n",
            "Epoch 458/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1001 - accuracy: 0.9873\n",
            "Epoch 459/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1201 - accuracy: 0.9848\n",
            "Epoch 460/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.1148 - accuracy: 0.9772\n",
            "Epoch 461/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0881 - accuracy: 0.9848\n",
            "Epoch 462/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1003 - accuracy: 0.9797\n",
            "Epoch 463/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1099 - accuracy: 0.9848\n",
            "Epoch 464/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0907 - accuracy: 0.9873\n",
            "Epoch 465/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1186 - accuracy: 0.9747\n",
            "Epoch 466/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0933 - accuracy: 0.9873\n",
            "Epoch 467/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0830 - accuracy: 0.9975\n",
            "Epoch 468/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1008 - accuracy: 0.9823\n",
            "Epoch 469/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1131 - accuracy: 0.9823\n",
            "Epoch 470/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1073 - accuracy: 0.9848\n",
            "Epoch 471/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0947 - accuracy: 0.9823\n",
            "Epoch 472/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1032 - accuracy: 0.9772\n",
            "Epoch 473/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1050 - accuracy: 0.9772\n",
            "Epoch 474/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1072 - accuracy: 0.9747\n",
            "Epoch 475/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1118 - accuracy: 0.9797\n",
            "Epoch 476/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0881 - accuracy: 0.9823\n",
            "Epoch 477/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1253 - accuracy: 0.9848\n",
            "Epoch 478/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1173 - accuracy: 0.9722\n",
            "Epoch 479/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0961 - accuracy: 0.9848\n",
            "Epoch 480/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1098 - accuracy: 0.9797\n",
            "Epoch 481/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0962 - accuracy: 0.9823\n",
            "Epoch 482/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0997 - accuracy: 0.9848\n",
            "Epoch 483/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1215 - accuracy: 0.9848\n",
            "Epoch 484/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0986 - accuracy: 0.9899\n",
            "Epoch 485/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0776 - accuracy: 0.9949\n",
            "Epoch 486/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1017 - accuracy: 0.9823\n",
            "Epoch 487/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0868 - accuracy: 0.9848\n",
            "Epoch 488/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9823\n",
            "Epoch 489/500\n",
            "8/8 [==============================] - 0s 10ms/step - loss: 0.1006 - accuracy: 0.9873\n",
            "Epoch 490/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1225 - accuracy: 0.9772\n",
            "Epoch 491/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1036 - accuracy: 0.9848\n",
            "Epoch 492/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0979 - accuracy: 0.9873\n",
            "Epoch 493/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0844 - accuracy: 0.9949\n",
            "Epoch 494/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.1010 - accuracy: 0.9899\n",
            "Epoch 495/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0802 - accuracy: 0.9975\n",
            "Epoch 496/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.0870 - accuracy: 0.9924\n",
            "Epoch 497/500\n",
            "8/8 [==============================] - 0s 7ms/step - loss: 0.0994 - accuracy: 0.9797\n",
            "Epoch 498/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1037 - accuracy: 0.9823\n",
            "Epoch 499/500\n",
            "8/8 [==============================] - 0s 8ms/step - loss: 0.1227 - accuracy: 0.9747\n",
            "Epoch 500/500\n",
            "8/8 [==============================] - 0s 9ms/step - loss: 0.0932 - accuracy: 0.9848\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdfc9840f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGfsLAnSnzPz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a6b98ecc-6b81-4751-d265-438188c3277b"
      },
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "\n",
        "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 82ms/step - loss: 0.0567 - accuracy: 1.0000\n",
            "\n",
            "\n",
            "Test Loss 0.05667649023234844, Test Accuracy 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1dQBQQ9n1g7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "5f32dca9-8efd-4aa4-d002-240725450f03"
      },
      "source": [
        "predictions = model.predict(test_data)\n",
        "\n",
        "# Show some results\n",
        "for prediction, hasCHD in zip(predictions[:10], list(test_data)[0][1][:10]):\n",
        "  print(\"Predicted CHD Diagnose Rate: {:.2%}\".format(prediction[0]),\n",
        "        \" | Actual outcome: \",\n",
        "        (\"Has CHD\" if bool(hasCHD) else \"Does not have CHD\"))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted CHD Diagnose Rate: 0.01%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 0.07%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 0.34%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 0.01%  | Actual outcome:  Has CHD\n",
            "Predicted CHD Diagnose Rate: 99.88%  | Actual outcome:  Has CHD\n",
            "Predicted CHD Diagnose Rate: 0.35%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 16.54%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 98.67%  | Actual outcome:  Has CHD\n",
            "Predicted CHD Diagnose Rate: 0.00%  | Actual outcome:  Does not have CHD\n",
            "Predicted CHD Diagnose Rate: 0.70%  | Actual outcome:  Does not have CHD\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}